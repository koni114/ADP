probs  <- runif(100) # 분류 알고리즘이 예측한 점수
labels <- as.factor(ifelse(probs > 0.5 & runif(100) < 0.4, "A", "B"))
library(ROCR)
pred <- prediction(probs, labels) # ROCR를 사용하기 위해 prediction 객체 생성
pred
auc  <- performance(pred, "auc")
auc
auc$y.values
auc[["y.values"]
auc[["y.values"]]
auc[["y.values"]]
auc@y.values
# 1. cvTools::cvFolds function
cvTools::cvFolds(
n,        # 데이터의 크기
K = 5,    # K겹 교차 검증
R = 1,    # R회 반복
type = c('random', 'consecutive', 'interleaved')
)
# 1. cvTools::cvFolds function
library(cvTools)
install.packages("cvTools")
cvFolds(10, 5, type = 'random')
# 1. cvTools::cvFolds function
library(cvTools)
cvFolds(10, 5, type = 'random')
cvFolds(10, 5, type = 'consecutive')
cvFolds(10, 5, type = 'interleaved')
cvFolds(10, 5, type = 'consecutive')
set.seed(2020)
cv <- cvFolds(NROW(iris), K = 10, R = 3)
cv
# 첫 번째 반복의 K = 1에서 거증 데이터로 사용해야 할 행의 번호 구하기
cv
cv
# 첫 번째 반복의 K = 1에서 거증 데이터로 사용해야 할 행의 번호 구하기
cv %>% filter(Fold == 1) %>% select(1)
# 첫 번째 반복의 K = 1에서 거증 데이터로 사용해야 할 행의 번호 구하기
library(dplyr)
cv %>% filter(Fold == 1) %>% select(1)
cv
data.frame(cv) %>% filter(Fold == 1) %>% select(1)
data.frame(cv)
cv
cv$which
data.frame(cv$subsets)
data.frame(cv$subsets) %>% filter(cv$which == 1) %>% select(x1)
data.frame(cv$subsets) %>% filter(cv$which == 1) %>% select(1)
data.frame(cv$subsets) %>% filter(cv$which == 1) %>% select(1) %>% unlist
data.frame(cv$subsets) %>% filter(cv$which == 1) %>% select(1) %>% unlist %>% as.numeric
validation_idx <- data.frame(cv$subsets) %>% filter(cv$which == 1) %>% select(1) %>% unlist %>% as.numeric
validation_idx
# K겹 교차 검증을 반복하는 전체 코드
library(foreach)
set.seed(719)
R = 3
K = 10
NROW
cv <- cvFolds(NROW(iris), K = K, R = R)
cv
1:R
cv$which
caret::createDataPartition(iris$Species, p=0.8)
parts <- caret::createDataPartition(iris$Species, p=0.8)
parts
table(iris[parts$Resample1, "Species"])
# 3. caret::createFolds
# K겹 교차 검증 지원
# iris Data 10중 교차 검증
createFolds(iris$Species, k =10)
createFolds
# 3. caret::createFolds
# K겹 교차 검증 지원
# iris Data 10중 교차 검증
createFolds(iris$Species, k = 10)
createFolds
# 3. caret::createFolds
# K겹 교차 검증 지원
# iris Data 10중 교차 검증
caret::createFolds(iris$Species, k = 10)
# 4. caret::createMultiFolds function
# K겹 교차 검증의 times 회 반복을 지원
caret::createMultiFolds(iris$Species, k = 10, times = 3)
d <- subset(iris, Species = "virginica" | Species == 'versicolor')
d
str(d)
d$Species <- factor(d$Species)
str(d)
m <- glm(Species ~ ., data = d, family = 'binomial')
d
m <- glm(Species ~ ., data = d, family = 'binomial')
m
str(d)
d <- subset(iris, Species = "virginica" | Species == 'versicolor')
str(d)
d$Species <- factor(d$Species)
str(d)
levels(d$Species)
levels(d$Species) <- c("virginica", "versicolor")
d <- subset(iris, Species = "virginica" | Species == 'versicolor')
d
str(d)
droplevels(d$Species)
d <- subset(iris, Species = "virginica" | Species == 'versicolor')
droplevels(d$Species)
d <- subset(iris, Species == "virginica" | Species == 'versicolor')
str(d)
droplevels(d$Species)
d$Species <- droplevels(d$Species)
d$Species
m <- glm(Species ~ ., data = d, family = 'binomial')
m
m
fitted(m)[c(1:5, 51:55)]
# fitted를 통해 모델이 적합된 값을 알 수 있음
f <- fitted(m)
as.numeric(d$Species)
ifelse(f > 0.5, 1, 0) == as.numeric(d$Species) - 1))
# fitted를 통해 모델이 예측한 값을 알 수 있음
f <- fitted(m)
ifelse(f > 0.5, 1, 0) == as.numeric(d$Species) - 1))
ifelse(f > 0.5, 1, 0) == as.numeric(d$Species - 1)
ifelse(f > 0.5, 1, 0) == as.numeric(d$Species) - 1
# 새로운 데이터에 대한 예측
predict(m, newdata = d[c(1, 10, 55),], type = 'response')
methods("predict")
library(nnet)
m <- multinom(Species ~., data = iris)
m
head(fitted(m))
# 새로운 관측값에 대한 예측 수행시 predict 함수 사용
predict(m, newdata = iris[c(1, 51, 101), ], type = 'class')
predict(m, newdata = iris[c(1, 51, 101), ], type = 'prob')
install.packages("rpart")
install.packages("rpart")
library(rpart)
m <- rpart(Species ~ ., data = iris)
m
plot(m, compress = T, margin = .2)
text(m, cex = 1.5)
install.packages("rpart.plot")
install.packages("rpart.plot")
install.packages("rpart.plot")
install.packages("rpart.plot")
library(rpart.plot)
prp(m, type = 4, extra = 2, digits = 3)
# 2.2 party::predict.BinaryTree function
party::predict.BinaryTree(
obj,
newdata,
# 예측 결과의 유형
# - response : 분류
# - node     : 잎사귀 노드의 ID
# - prob     : 각 분류에 대한 확률
type = c('response', 'node', 'prob')
)
install.packages("party")
library(party)
m <- ctree(Species ~.,  data = iris)
m
plot(m)
# 결과 해석
# 결과를 통해 어느 노드에서 분류가 잘 되지 않았는지 확인 가능
plot(m)
#######################
## 04. randomForest  ##
#######################
# 데이터를 복원 추출로 꺼내서 subsample을 생성
# 자식 노드를 나눌 때 전체 변수가 아니라 일부 변수만 대상으로 하여 기준을 찾음
library(randomForest)
library(randomForest)
m <- randomForest(Species ~., data = iris)
m
head(predict(m, newdata = iris))
# 일반적으로 formula로 지정시 많은 메모리를 필요로 하고, 속도가 더 느리다고 알려져 있음
randomForest(x = iris[,1:4], y = iris[,5])
변수 중요도
# 변수의 정확도와 노드 불순도 개선에 얼마만큼 기여하는지로 측정
# 변수 선택 중 필터 방법
# randomForest 변수 중요도
# 변수의 정확도와 노드 불순도 개선에 얼마만큼 기여하는지로 측정
# 변수 선택 중 필터 방법
# 이렇게 나온 변수 중요도를 통해 다른 모델링 시 적용할 수 있음
m <- randomForest(Species ~., data = iris, importance = T)
m
varImpPlot(m)
importance(m)
importance
varImpPlot(m)
# 랜덤 포레스트를 개선한 변수 선택 방법
grid <- expand.grid(ntree = (10, 100, 200), mtry = c(3, 4))
grid
grid <- expand.grid(ntree = (10, 100, 200), mtry = c(3, 4))
grid <- expand.grid(ntree = c(10, 100, 200), mtry = c(3, 4))
grid <- expand.grid(ntree = c(10, 100, 200), mtry = c(3, 4))
grid
## 파라미터 튜닝을 통한 모델 성능 평가(3번 반복)
library(cvtools)
## 파라미터 튜닝을 통한 모델 성능 평가(3번 반복)
library(cvTools)
library(foreach)
library(randomForest)
set.seed(100)
R = 3
K = 10
cv = cvFolds(NROW(iris), K = K, R = R)
cv
grid = expand.grid(ntree = c(10, 100, 200), mtry = c(3, 4))
grid
R = 3
cv   = cvFolds(NROW(iris), K = K, R = R)
K = 10
R = 3
cv   = cvFolds(NROW(iris), K = K, R = R)
grid = expand.grid(ntree = c(10, 100, 200), mtry = c(3, 4))
result <- foreach(g = 1:NROW(grid), .combine = rbind) %do% {
foreach(r=1:R, .combine = rbind) %do% {
foreach(k=1:K, .combine=rbind)  %do% {
validation_idx <- cv$subsets[which(cv$which == k), r]
train          <- iris[-validation_idx, ]
validation     <- iris[validation_idx , ]
m <- randomForest(Species ~ .
, data =train
, ntree = grid[g, "ntree"]
, mtry  = grid[g, "mtry"])
# 예측
predicted <- predict(m, newdata = validation)
precision <- sum(predicted == validation$Species) / NROW(predicted)
return(data.frame(g = g, precision = precision))
}
}
}
result
cv$which
# g 값마다 묶어 평균 구하기
result
head(result)
# g 값마다 묶어 평균 구하기
result %>% group_by(g) %>% summarise_all(list(mean = ~ mean(., na.rm = T)))
# g 값마다 묶어 평균 구하기
library(dplyr)
result %>% group_by(g) %>% summarise_all(list(mean = ~ mean(., na.rm = T)))
nnet::nnet(
x, y,     # 데이터
weights,  # 데이터에 대한 가중치
size,     # 은닉층 노드의 수
Wts,      # 초기 가중치 값. 생략할 경우 임의의 값이 가중치로 할당
mask,     # 최적화할 파라미터. 가본값은 모든 파라미터의 최적화
# TRUE면 활성 함수로 y = ax+b 같은 유형의 선형 출력(linear output)이 사용
# FALSE면 로지스틱 함수(즉, 시그모이드 함수)가 활성함수로 사용
)
nnet::nnet(
x, y,     # 데이터
weights,  # 데이터에 대한 가중치
size,     # 은닉층 노드의 수
Wts,      # 초기 가중치 값. 생략할 경우 임의의 값이 가중치로 할당
mask,     # 최적화할 파라미터. 가본값은 모든 파라미터의 최적화
# TRUE면 활성 함수로 y = ax+b 같은 유형의 선형 출력(linear output)이 사용
# FALSE면 로지스틱 함수(즉, 시그모이드 함수)가 활성함수로 사용
linout = F,
# 모델 학습 시 모델의 출력과 원하는 값을 비교할 때 사용할 함수. TRUE면 엔트로피.
)
nnet
library("nnet", lib.loc="C:/Program Files/R/R-3.6.1/library")
nnet::nnet(
x, y,     # 데이터
weights,  # 데이터에 대한 가중치
size,     # 은닉층 노드의 수
Wts,      # 초기 가중치 값. 생략할 경우 임의의 값이 가중치로 할당
mask,     # 최적화할 파라미터. 가본값은 모든 파라미터의 최적화
# TRUE면 활성 함수로 y = ax+b 같은 유형의 선형 출력(linear output)이 사용
# FALSE면 로지스틱 함수(즉, 시그모이드 함수)가 활성함수로 사용
linout = F,
# 모델 학습 시 모델의 출력과 원하는 값을 비교할 때 사용할 함수. TRUE면 엔트로피 F면 SSE 사용
entropy = F,
# 출력에서 소프트멕스 사용 여부
softmax = F,
# 가중치의 최대 개수로 기본값은 1000, 모델이 복잡해 가중치의 수가 많다면 이 값을 증가시켜야 함
MaxNWts = 1000
)
nnet::nnet(
x, y,     # 데이터
weights,  # 데이터에 대한 가중치
size,     # 은닉층 노드의 수
Wts,      # 초기 가중치 값. 생략할 경우 임의의 값이 가중치로 할당
mask,     # 최적화할 파라미터. 가본값은 모든 파라미터의 최적화
# TRUE면 활성 함수로 y = ax+b 같은 유형의 선형 출력(linear output)이 사용
# FALSE면 로지스틱 함수(즉, 시그모이드 함수)가 활성함수로 사용
linout = F,
# 모델 학습 시 모델의 출력과 원하는 값을 비교할 때 사용할 함수. TRUE면 엔트로피 F면 SSE 사용
entropy = F,
# 출력에서 소프트멕스 사용 여부
softmax = F,
# 가중치의 최대 개수로 기본값은 1000, 모델이 복잡해 가중치의 수가 많다면 이 값을 증가시켜야 함
MaxNWts = 1000
)
library(nnet)
m <- nnet(Species ~., data = iris, size = 3)
m
predict(m, newdata = iris)
predict(m, newdata = iris, type = 'class')
iris
iris[, c(5)]
nnet::class.ind(iris[,5])
nnet(
x = iris[,c(1:4)],
y = nnet::class.ind(iris[,c(5)])
)
nnet(
x = iris[,c(1:4)],
y = nnet::class.ind(iris[,c(5)]),
size = 3,
softmax = T
)
m2 <- nnet(
x = iris[,c(1:4)],
y = nnet::class.ind(iris[,c(5)]),
size = 3,
softmax = T
)
predict(m2, newdata = iris[,1:4], type = 'class')
install.packages("ksvm")
install.packages("e1071")
install.packages("kernlab")
library(kernlab)
kernlab::ksvm(Species ~., data = iris)
svm_model <- kernlab::ksvm(Species ~., data = iris)
svm_model
svm_model
head(predict(m, newdata = iris))
svm_model <- kernlab::ksvm(Species ~., data = iris, kernel = 'vanilladot')
svm_model
head(predict(m, newdata = iris))
svm_model <- kernlab::ksvm(Species ~., data = iris, kernel = 'polydot', kpar = list(degree = 3))
svm_model
tune(svm, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune
e1071::best.svm(svm, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
e1071::best.tune(svm_model, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
svm_model <- kernlab::ksvm(Species ~., data = iris)
e1071::best.tune(svm_model, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
e1071::tune(svm_model, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
e1071::tune(method = svm_model, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
2^(2:4)
e1071::svm(Species ~., data = iris)
svm <- e1071::svm(Species ~., data = iris)
e1071::tune(method = svm, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
svm <- e1071::svm(Species ~., data = iris)
svm
e1071::tune(method = svm, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
e1071::tune(method = svm, train.x = Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
e1071::tune(method = svm, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
svm <- e1071::svm(Species ~., data = iris)
e1071::tune(method = svm, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
svm
e1071::tune(svm, Species ~., data= iris, gamma = 2^(-1:1), cost = 2^(2:4))
e1071::tune(svm, Species ~., data= iris, gamma = **2^(-1:1)**, cost = **2^(2:4)**)
2^(-1:1)
e1071::tune(svm, Species ~., data= iris, gamma = c(1/2, 1, 2), cost = c(4, 8, 16))
e1071::tune(svm, Species ~., data  = iris)
e1071::tune(svm, train.x = iris[,c(1:4)], train.y = iris[,c(5)], gamma = c(1/2, 1, 2), cost = c(4, 8, 16))
svm
iris[,c(5)]
e1071::tune(svm, train.x = iris[,c(1:4)], train.y = iris[,c(5)])
svm <- e1071::svm(Species ~., data = iris)
svm
e1071::tune(svm, train.x = iris[,c(1:4)], train.y = iris[,c(5)])
svm_model <- kernlab::ksvm(Species ~., data = iris)
e1071::tune(svm_model, train.x = iris[,c(1:4)], train.y = iris[,c(5)])
?e1071::tune
tune.svm(Species~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune.svm
tune(svm, Species~., data = iris,
ranges = list(gamma = 2^(-1:1), cost = 2^(2:4)),
tunecontrol = tune.control(sampling = "fix")
)
e1071::tune(svm, Species~., data = iris,
ranges = list(gamma = 2^(-1:1), cost = 2^(2:4)),
tunecontrol = tune.control(sampling = "fix")
)
library(e1071)
e1071::tune(svm, Species~., data = iris,
ranges = list(gamma = 2^(-1:1), cost = 2^(2:4)),
tunecontrol = tune.control(sampling = "fix")
)
svm
svm <- e1071::svm(Species ~., data = iris)
e1071::tune(svm, train.x = iris[,c(1:4)], train.y = iris[,c(5)])
e1071::tune(svm, Species~., data = iris,
ranges = list(gamma = 2^(-1:1), cost = 2^(2:4)),
tunecontrol = tune.control(sampling = "fix")
)
svm
svm_model <- kernlab::ksvm(Species ~., data = iris)
svm <- e1071::svm(Species ~., data = iris)
svm
e1071::tune("svm", train.x = iris[,c(1:4)], train.y = iris[,c(5)])
result <- e1071::tune("svm", train.x = iris[,c(1:4)], train.y = iris[,c(5)])
result
attribute(result)
attributes(result)
result$best.parameters
result <- e1071::tune("svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
result
result$best.parameters
result <- e1071::tune("svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
result
attributes(result)
result$best.parameters
result <- e1071::tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
result
attributes(result)
result$best.parameters
result <- e1071::tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
result
result$best.parameters
result$best.performance
result$method
result$train.ind
result$best.model
result
svm    <- e1071::svm(Species ~., data = iris)
result <- e1071::tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
result
2^(-1:1)
2^(2:4)
tune
result <- tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
result
attributes(result)
result$best.parameters
result$best.performance
e1071::svm(Species ~., data = iris)
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
result
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
tune(method = "svm", Species ~., data = iris, gamma = 2^(-1:1), cost = 2^(2:4))
library(mlbench)
data(BreastCancer)
BreastCancer
x <- caret::upSample(subset(BreastCancer, select = -Class), BreastCancer$Class)
x
table(x$Class)
# 2), 3) 노드는 왼쪽 노드
# 이 기준을 만족하는 노드는 setosa 뿐, 노드에는 총 50개의 데이터가 속함
m <- rpart(Species ~ ., data = iris)
library(rpart)
# 2), 3) 노드는 왼쪽 노드
# 이 기준을 만족하는 노드는 setosa 뿐, 노드에는 총 50개의 데이터가 속함
m <- rpart(Species ~ ., data = iris)
plot(m, compress = T, margin = .2)
text(m, cex = 1.5)
install.packages("rpart.plot")
# install.packages("rpart.plot")
library(rpart.plot)
prp(m, type = 4, extra = 2, digits = 3)
prp(  m
, type   = 4
, extra  = 4
, digits = 3)
prp(  m
, type   = 4
, extra  = 2
, digits = 3)
prp(  m
, type   = 4
, extra  = 0
, digits = 3)
prp(  m
, type   = 4
, extra  = 3
, digits = 3)
prp(  m
, type   = 4
, extra  = 4
, digits = 3)
prp(  m
, type   = 4
, extra  = 5
, digits = 3)
prp(  m
, type   = 4
, extra  = 3
, digits = 3)
prp(  m
, type   = 4
, extra  = 2
, digits = 3)
prp(  m
, type   = 4
, extra  = 3
, digits = 3)
prp(  m
, type   = 4
, extra  = 2
, digits = 3)
m <- ctree(Species ~.,  data = iris)
# 결과 해석
# 결과를 통해 어느 노드에서 분류가 잘 되지 않았는지 확인 가능
# 안보이는 종이 있는데, levels 함수의 순서가 그래프 x축의 순서
plot(m)
source('C:/r/chapter10_분류알고리즘_2.R', encoding = 'UTF-8')
cvFolds(10, 5, type = 'random')
