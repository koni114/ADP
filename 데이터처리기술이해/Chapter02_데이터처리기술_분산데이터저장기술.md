## Chapter_02 데이터 처리 기술 - 분산 데이터 저장 기술
---
> * 최근 대량 데이터를 처리할 수 있는 대규모 클러스터 시스템을 필요
> * 기존의 아키텍처는 고가의 마스터 서버에서 많은 역할을 수행하는 중앙 집중형 방식
> why? 관리가 수월하기 때문
> * GFS나 BigTable 같은 플랫폼은 저가의 PC급 서버들로 클러스터를 구성해 마스터 서버의 역할을 축소
> * 이러한 아키텍처는 현재 대용량 처리 아키텍처로 적합
> * 최근 전통적인 DBMS 에서도 데이터의 볼륨이 커짐에 따라 분산 기술이 필요하게 됨
> * 이처럼 성능 저하를 막는 동시에 데이터의 가용성을 높이기 위한 솔루션으로 PDS(partitioning Data System) 이나 클러스터링을 이용한 데이터 통합 기술도 있음

### 분산 파일 시스템
> * 비대칭형(asymmetric) 클러스터 파일 시스템
>   +  성능과 확장성, 가용성 면에서 적합한 분산 파일 시스템 구조
>   + 파일 메타데이터를 관리하는 전용 서버를 별도로 둠으로써 메타 데이터에 접근하는 경로와 데이터에 접근하는 경로를 분리
>   + 이를 통해 파일 입출력 성능을 높이면서 확장과 안정적인 파일 서비스 제공
>   + 메타데이터의 부하 우려, sigle-of-failure 발생 우려
>   + 구글에서 GFS에 대한 논문 발표 후 관심 폭발
>   + 최근 Yahoo에서 적극적으로 지원하여 Aphache의 하둡 파일 시스템까지 등장

### 구글 파일 시스템(GFS)
> * 구글의 대규모 클러스터 서비스 플랫폼의 기반이 되는 파일 시스템
> * 다음과 같은 가정을 토대로 설계
>   + 저가형 서버로 구성된 환경으로, 고장이 빈번히 발생할 수 있음
>   + 대부분의 파일은 대용량으로 가정. 대용량 파일을 효과적으로 관리할 수 있는 방안 필요
>   + 작업 부하는 주로 연속적으로 많은 데이터를 읽는 연산, 임의의 영역에서 적은 데이터를 읽는 연산임
>   + 파일 쓰기 연산은 순차적으로 데이터를 추가하며, 파일에 대한 갱신은 거의 일어나지 않음
>   + 여러 클라이언트에서 동시에 동일한 파일에 데이터를 추가하는 환경에서 동기화 오버헤드를 최소화 할 수 있는 방법 요구
>   + 낮은 응답 지연시간보다 높은 처리율이 중요
> * GFS는 <b>클라이언트, 마스터, chunk 서버</b>들로 구성


> ![img](C://데이터분석자료/ADP준비자료/GFS.jpg)

>  * POSIX 인터페이스를 지원하지 않고, 자체 인터페이스 지원
>  * 여러 클라이언트에서 원자적인 데이터 추가 연산을 지원하기 위한 인터페이스 지원
>  * 파일은 고정된 크기의 chuck으로 나누어 chunk 서버들에 분산 저장됨
>  * 각 chunk에 대한 여러 개의 복제본도 chunk 서버에 분산 저장
>  * 클라이언트들은 파일에 접근하기 위하여 우선적으로 마스터로부터 파일의 chunk가 저장된 chunk 서버의 위치와 핸들을 받아옴
>  * 이어서 직접 chunk 서버로 파일 데이터를 요청
>  * GFS의 마스터는 단일 마스터 구조로, 모든 메타 데이터를 메모리상에서 관리
>  * 기본 64MB 지정
>  * 해쉬 테이블 구조 등을 사용함으로써 메모리상에서 보다 효율적인 메타데이터 처리 지원
>  * heartbeat 메세지를 이용하여 chunk 서버에 저장된 chunk들의 상태 체크하고, 필요시 복제
>  * chunk 서버는 로컬 디스크에 chunk를 저장, 관리하면서 클라이언트로부터 chunk 입출력 요청을 처리
>  * chunk는 마스터에 의해 생성/삭제 가능, 유일한 식별자에 의해 구분

### 하둡 분산 파일 시스템
> * 구글 파일 시스템과 아키텍처 사상을 그대로 구현한 클로닝(cloning) 프로젝트
> * 하나의 NameNode와 여러개의 DataNode로 구성
> * NameNode는 파일 시스템의 이름 공간을 관리하면서 클라이언트로부터의 파일 접근 요청을 처리
> * 파일데이터는 블록 단위로 나뉘어 DataNode에 분산, 저장
> * 블록들은 가용성을 보장하기 위해 다시 복제, 저장
> * HDFS에서 파일은 한 번 쓰이면 변경되지 않음. 따라서 HDFS는 데이터에 대한 스트리밍 접근을 요청하고 배치 작업에 적합한 응용을 대상으로 함
> * 클라이언트, 네임노드, 데이터노드 간 통신을 위하여 RPC 사용

### 러스터(Lustre)
> * Cluster file system Inc에서 개발한 객체 기반 클러스터 파일 시스템
> * 클라이언트 파일 시스템, 메타데이터 서버, 객체 저장 서버들로 구성
> * 계층화된 모듈 구조로 TCP/IP, 인피니밴드, 미리넷 같은 네트워크 지원
> * 구성 요소 특징
>   + ① 클라이언트 파일 시스템
> 리눅스 VFS에서 설치할 수 있는 시스템  
> 메타데이터 서버와 객체 저장 서버들과 통신하면서 클라이언트 응용에 파일 시스템 인터페이스 제공
>   + ② 메타데이터 서버
> 파일 시스템의 이름 공간과 파일에 대한 메타데이터를 관리
>   + ③ 객체 저장 서버
> 파일 데이터 저장  
> 클라이언트로부터 객체 입출력 요청 처리
> 객체 저장 서버들에 스트라이핑 되어 분산, 저장
> * 러스터는 유닉스 시맨틱을 제공하면서 파일 메타데이터에 대해서는 Write Back Cashe를 지원
> * 클라이언트에서 메타데이터 변경에 대한 갱신 레코드를 생성하고 나중에 메타데이터 서버에 전달
> * 메타데이터의 동시 접근이 적으면 클라이언트 캐시를 이용한 Write Back Cashe 사용
> * 메타데이터의 동시 접근이 많으면 클라이언트 캐시를 사용함으로써 동시 접근에 대한 오버헤드를 줄임
> * 파일 메타데이터와 파일 데이터에 대한 동시성 제어를 위해 별도의 잠금을 사용
>




### 용어 정리
> * RPC(Remote Procedure Call)
> 한 프로그램이 다른 컴퓨터에 위치하고 있는 프로그램에 서비스를 요청하는 프로토콜


> * 스트림(Stream)
> 일반적으로 데이터,패킷,비트 등의 일련의 연속성을 갖는 흐름

> * 스트라이핑(Striping)
> 스트라이핑은 성능 향상을 위해 단일 파일과 같은 논리적으로 연속된 데이터 세그먼트들이, 물리적으로 여러 개의 장치, 즉 디스크 드라이브 등에 라운드로빈 방식으로 나뉘어 기록될 수 있는 것

> * Write Back 정책
> 이 방식은 CPU에서 메모리에 대한 쓰기 작업 요청 시 캐시에서만 쓰기 작업과 그 변경 사실을 확인할 수 있는 표시를 하여 놓은 후 캐시로부터 해당 블록의 내용이 제거될 때 그 블록을 메인 메모리에 복사함으로써 메인 메모리와 캐시의 내용을 동일하게 유지하는 방식  
