## Chapter_02 데이터 처리 기술 - 분산 데이터 저장 기술
---
> * 최근 대량 데이터를 처리할 수 있는 <b>대규모 클러스터 시스템</b>을 필요  

> * 기존의 아키텍처는 고가의 마스터 서버에서 많은 역할을 수행하는 중앙 집중형 방식  
> why? 관리가 수월하기 때문

> * GFS나 BigTable 같은 플랫폼은 저가의 PC급 서버들로 클러스터를 구성해 마스터 서버의 역할을 축소  

> * 이러한 아키텍처는 현재 대용량 처리 아키텍처로 적합  

> * 최근 전통적인 DBMS 에서도 데이터의 볼륨이 커짐에 따라 분산 기술이 필요하게 됨  

> * 이처럼 성능 저하를 막는 동시에 데이터의 가용성을 높이기 위한 솔루션으로 PDS(partitioning Data System) 이나 클러스터링을 이용한 데이터 통합 기술도 있음  

### 분산 파일 시스템
> * 클라이언트 측에서 서버에 저장된 데이터에 접근, 마치 자신에게 저장되어 있는 데이터인 것 처럼 처리할 수 있는 클라이언트/서버 기반 Application  

> * 기존의 NFS(Network File System)와 같은 단순한 클라이언트/서버 수준의 분산 파일 시스템으로는 시스템 성능과 확장에 한계가 이른다.  

> * <b> 비대칭형(asymmetric) 클러스터 파일 시스템 </b> 대두  

>   +  성능과 확장성, 가용성 면에서 적합한 분산 파일 시스템 구조  

>   + <b> 파일 메타데이터를 관리하는 전용 서버를 별도로 둠</b>으로써 메타 데이터에 접근하는 경로와 데이터에 접근하는 경로를 분리  

>   + 이를 통해 파일 입출력 성능을 높이면서 확장과 안정적인 파일 서비스 제공  

>   + 메타데이터의 부하 우려, single-of-failure 발생 우려  

>   + 구글에서 GFS에 대한 논문 발표 후 관심 폭발  

>   + 최근 Yahoo에서 적극적으로 지원하여 Apache의 하둡 파일 시스템까지 등장  

### 구글 파일 시스템(GFS)
> * 구글의 대규모 클러스터 서비스 플랫폼의 기반이 되는 파일 시스템  

> * 비대칭형 파일 시스템 기반  

> * 다음과 같은 가정을 토대로 설계  
>   + 저가형 서버로 구성된 환경으로, 고장이 빈번히 발생할 수 있음
>   + 대부분의 파일은 대용량으로 가정. 대용량 파일을 효과적으로 관리할 수 있는 방안 필요
>   + 작업 부하는 주로 연속적으로 많은 데이터를 읽는 연산, 임의의 영역에서 적은 데이터를 읽는 연산임
>   + 파일 쓰기 연산은 순차적으로 데이터를 추가하며, 파일에 대한 갱신은 거의 일어나지 않음
>   + <b> 여러 클라이언트에서 동시에 동일한 파일에 데이터를 추가하는 환경에서 동기화 오버헤드를 최소화 할 수 있는 방법 요구 </b>
>   + 낮은 응답 지연시간보다 높은 처리율이 중요

> * GFS는 <b>클라이언트, 마스터, chunk 서버</b> 들로 구성  

> ![img](C://데이터분석자료/ADP준비자료/GFS.jpg)  

>  * POSIX 인터페이스를 지원하지 않고, 자체 인터페이스 지원  

>  * 여러 클라이언트에서 원자적인 데이터 추가 연산을 지원하기 위한 인터페이스 지원  

>  * 파일은 고정된 크기의 chuck으로 나누어 chunk 서버들에 분산 저장됨  

>  * 각 chunk에 대한 여러 개의 복제본도 chunk 서버에 분산 저장  

>  * 클라이언트들은 파일에 접근하기 위하여 우선적으로 마스터로부터 파일의 chunk가 저장된 chunk 서버의 위치와 핸들을 받아옴  

>  * 이어서 직접 chunk 서버로 파일 데이터를 요청  

>  * GFS의 마스터는 단일 마스터 구조로, 모든 메타 데이터를 메모리상에서 관리  

>  * 기본 64MB 지정  

>  * 해쉬 테이블 구조 등을 사용함으로써 메모리상에서 보다 효율적인 메타데이터 처리 지원  

>  * heartbeat 메세지를 이용하여 chunk 서버에 저장된 chunk들의 상태 체크하고, 필요시 복제  

>  * chunk 서버는 로컬 디스크에 chunk를 저장, 관리하면서 클라이언트로부터 chunk 입출력 요청을 처리  

>  * chunk는 마스터에 의해 생성/삭제 가능, 유일한 식별자에 의해 구분

### 하둡 분산 파일 시스템
> * <b> 구글 파일 시스템과 아키텍처 사상을 그대로 구현한 클로닝(cloning) 프로젝트 </b>  

> *  하나의 <b>NameNode와 여러개의 DataNode로</b> 구성  

> * NameNode는 파일 시스템의 이름 공간을 관리하면서 클라이언트로부터의 파일 접근 요청을 처리  

> * 파일데이터는 블록 단위로 나뉘어 DataNode에 분산, 저장  

> * 블록들은 가용성을 보장하기 위해 다시 복제, 저장  

> * HDFS에서 파일은 한 번 쓰이면 변경되지 않음. 따라서 HDFS는 데이터에 대한 스트리밍 접근을 요청하고 배치 작업에 적합한 응용을 대상으로 함  

> * NameNode는 DataNode들로부터 HeartBeat를 주기적으로 받으면서 DataNode들의 상태를 체크  

> * 클라이언트, 네임노드, 데이터노드 간 통신을 위하여 TCP/IP 네트쿼으 상에 RPC 사용  

### 러스터(Lustre)
> * Cluster file system Inc에서 개발한 객체 기반 클러스터 파일 시스템  

> * <b> 클라이언트 파일 시스템, 메타데이터 서버, 객체 저장 서버들</b>로 구성  

> * 계층화된 모듈 구조로 TCP/IP, 인피니밴드, 미리넷 같은 네트워크 지원  

> * 구성 요소 특징  
>   + ① 클라이언트 파일 시스템  
> 리눅스 VFS에서 설치할 수 있는 시스템  
> 메타데이터 서버와 객체 저장 서버들과 통신하면서 클라이언트 응용에 파일 시스템 인터페이스 제공  
>   + ② 메타데이터 서버  
> 파일 시스템의 이름 공간과 파일에 대한 메타데이터를 관리  
>   + ③ 객체 저장 서버  
> 파일 데이터 저장  
> 클라이언트로부터 객체 입출력 요청 처리  
> 객체 저장 서버들에 스트라이핑 되어 분산, 저장  

> * 러스터는 유닉스 시맨틱을 제공하면서 파일 메타데이터에 대해서는 Write Back Cashe를 지원  

> * 클라이언트에서 메타데이터 변경에 대한 갱신 레코드를 생성하고 나중에 메타데이터 서버에 전달  

> * 메타데이터의 동시 접근이 적으면 클라이언트 캐시를 이용한 Write Back Cashe 사용  

> * 메타데이터의 동시 접근이 많으면 클라이언트 캐시를 사용함으로써 동시 접근에 대한 오버헤드를 줄임  

> * 파일 메타데이터와 파일 데이터에 대한 동시성 제어를 위해 별도의 잠금을 사용  
>

### 데이터베이스 클러스터
> * 데이터를 통합 할 때, 성능 차원과 가용성을 높이기 위해 데이터베이스 차원의 파티셔닝 또는 클러스터링을 이용  

> * DB 파티셔닝 구현 시 이점  
>   + 파티션 사이의 병렬 처리를 통한 빠른 데이터 검색 및 처리 성능  
>   + 성능의 선형적인 증가  
>   + 파티션에 장애 발생 시에 서비스가 중단되지 않는 고가용성 확보  

> * <b>DB 시스템을 구현하는 형태에 따라 단일 서버 내에 파티셔닝과 다중 서버 사이의 파티셔닝</b>으로 구분  

> * 리소스 공유 관점에서는 <b>공유 디스크와 무공유</b>로 구분  
>   + <b> 무공유 디스크 </b>  
> 각 데이터베이스 인스턴스는 자신이 관리하는 데이터 파일을 자신의 로컬 디스크에 저장  
> 파일들은 노드간에 공유되지 않음  
> 노드 확장에 제한이 없음  
> 각 노드에 장애가 발생할 경우를 대비해 별도의 폴트톨러런스를 구성해야 함    
> 대부분의 데이터베이스 클러스터가 무공유 방식 채택  
>   + <b> 공유 디스크 </b>  
> 공유 디스크 클러스터에서 데이터 파일은 모든 DB 인스턴스 노드들과 공유  
> 각 인스턴스는 모든 데이터에 접근 가능  
> 높은 수준의 폴트톨러런스가 가능. 하나의 노드만 살아있어도 서비스가 가능  
> Oracle RAC가 공유 디스크 방식을 채택하고 있음  

> ##### * Oracle RAC DB 서버  
> * Oracle DB 서버는 클러스터의 모든 노드에서 실행  

> * Data는 공유 스토리지에 저장  

> * 클러스터의 모든 노드는 DB의 Table에 동등하게 엑세스  

> * 특정 노드가 데이터를 소유하는 개념은 없어 실제로는 데이터 파티셔닝이 필요 없지만 성능을 위해 파티셔닝함  

> * 응용 프로그램은 클러스터의 특정 노드가 아니라 RAC 클러스터에 연결  

> * 이러한 RAC는 클러스터의 모든 노드에 로드를 고르게 분산  

> * 한 노드가 장애를 일으키면 나머지 노드가 계속 실행(가용성)  

> * 추가 처리 성능이 필요하면 Application이나 DB 수정 없이 새 노드를 클러스터에 쉽게 추가 가능  

> * 하드웨어 비용 절감  

>  ##### * IBM DB2 ICE(Integrated Cluster Environment)  
>
> * CPU, 메모리, 디스크를 파티션별로 독립적으로 운영하는 무공유 방식의 클러스터링 지원  

> * 애플리케이션은 여러 파티션에 분산된 DB를 하나의 DB로 보게 됨  

> * 데이터가 어떤 파티션에 존재하는지 알 필요가 없음  

> * 데이터와 사용자 증가시, 애플리케이션 수정 없이 기존 시스템에 노드를 추가하고 데이터를 재분배해 시스템의 성능과 용량을 일정하게 유지  

> * 노드의 장애 발생 시 fail-over 매커니즘이 필요  

> * 따라서 DB2를 이용하여 클러스터를 구성할 때에도 가용성을 보장하기 위해 공유 디스크 방식 이용  

> ##### * MicroSoft SQL Server

> * 연합 DB 형태로, 여러 노드로 확장할 수 있는 기능을 제공  

> * 연합 DB는 디스크를 공유하지 않는 독립된 서버에서 실행되는 서로 다른 DB간의 논리적인 결합  

> ![img](C://데이터분석자료/ADP준비자료/MSSQLServer.jpg)

>  * 데이터는 관련된 서버들로 수평적으로 분할  

>  * 테이블을 논리적으로 분리해 물리적으로는 분산된 각 노드에 생성하고 각 노드의 DB 인스턴스 사이에 링크를 구성한 후 모든 파티션에 대해 UNION ALL을 이용해 논리적인 VIEW를 구성하는 방식  
--> 이를 DVP(DIstributed Partitioned View) 라고 함  

> * Global schema 정보가 없기 때문에 모든 노드를 엑세스해야 함  

> * Fail-over 매커니즘으로 Active-Standby 방법 사용  

##### *  MySQL

> * MySQL 클러스터는 무공유 구조에서 메모리 기반 DB 클러스터링을 지원  

> * HW 및 SW를 요구하지 않고 병렬 서버구조로 확장 가능  

> * 관리 노드, 데이터 노드, MySQL 노드로 구성  
>   + 관리 노드 : 클러스터 관리하는 노드. 클러스터 시작과 재구성 시에만 관여  
>   + 데이터 노드 : 클러스터의 데이터를 저장하는 노드  
>   + MySQL 노드 : 클러스터 데이터에 접근을 지원  

> * 가용성을 높이기 위해 데이터를 다른 노드에 복제하여 특정 노드에 장애가 생기더라도 지속적인 서비스 가능  

> * 장애 복구가 된 노드는 자동으로 변경된 데이터에 대해 동기화 수행  

> * 데이터는 동기화 방식으로 복제되며, 데이터 노드 간에는 별도의 네트워크를 구성  
>   + domain : 관계형 DB에서 Table과 동일한 개념  
>   + Items : 레코드와 동일한 개념  
>   + Attribute : 컬럼과 동일한 개념. 미리 정의할 필요가 없음. Name, Value 쌍으로 데이터를 저장  
### NoSQL
> * Key-Value 형태로 자료 저장  

> * 빠르게 조회할 수 있는 자료 구조를 제공하는 <b> 저장소 </b>  

> * Join 연산은 지원하지 않지만 대용량 데이터와 대규모 확장성 제공  
##### * 구글 빅테이블(Google BigTable)
> * 데이터 서비스가 아닌 구글 내부적으로 사용하는 데이터 저장소  

> * multi-dimension sorted hash map을 파티션하여 분산 저장하는 저장소  

> * 테이블 내의 모든 데이터는 row-key의 사전적 순서로 정렬, 저장  

> * row는 n개의 column-family를 가질 수 있으며 column-key, value, timestamp 의 형태로 데이터 저장  

> * 정렬 기준은 rowkey + columnkey +  timestamp 가 됨  

> * 테이블의 파티션은 row-key 이용, 분리된 파티션은 분산된 노드에서 서비스  

> * 분리된 파티션을 Tablet이라 함  

> * 마스터(Master)는 장애가 발생한 노드에서 서비스되던 Tablet 을 다른 노드로 할당  

> * 공유디스크 방식을 따름  

> * Chubby:  마스터 서버 장애 확인 후, 다른 노드로 변경 역할 수행  

> * 데이터 저장소를 위해 별도 클러스터를 구성하는 것이 아니라, 파일시스템, Map & Reduce  컴퓨팅 클러스터 위에 구성됨  

> * 한 번에 하나의 도메인에 대해서만 쿼리 수행해야 함  

##### * 마이크로소프트 SSDS
> * Azure 로 플랫폼 명칭이 바뀐듯 하다

> * 고가용성 보장

> * 데이터 모델은 <b> 컨테이너와 엔티티</b>로 구성  
>   + 컨테이너  : 테이블과 유사한 개념이지만, 여러 개의 엔티티를 저장할 수 있음  
>   + 엔티티    : 레코드와 유사한 개념  

> * 컨테이너는 여러 노드에 분산, 관리 됨  

> * 쿼리는 하나의 컨테이너를 대상으로 함  

> * 컨테이너의 생성/삭제, 엔티티의 생성/삭제, 조회, 쿼리 등의 API 제공  

> * SOAP/REST 기반의 프로토콜 지원  

##### * 아마존 SimpleDB
> * 아마존(Amazon)의 데이터 서비스 플랫폼  

> * 웹 어플리케이션에서 사용하는 데이터 실시간 처리를 지원  

> * 주로 아마존의 다른 플랫폼 서비스와 같이 사용  

> * 하나의 데이터에 대해 여러 개의 복제본을 유지하는 방식으로 가용성을 높임  

> * 복제본 간의 consistency를 고려할 때, 'Eventual Consistency' 정책을 취함  

> * 관계형 데이터 모델과 표준 SQL을 지원하지 않으며, <b>전용 쿼리 언어</b>를 이용하여 데이터를 조회  

> * 데이터 모델은 Domain, Item, Attribute, Value로 구성되며 스키마가 없는 구조  

### 용어 정리
> * schema
> 스키마는 데이터베이스의 구조와 제약 조건에 관한 전반적인 명세를 기술한 메타데이터의 집합이다.

> * eventual consistency
> 트랜잭션 종료 후 데이터는 모든 노드에 즉시 반영하지 않고 초 단위로 지연되어 동기화 되는 것을 의미

> * Fault Tolerance
> 시스템 내에 결함 또는 장애가 발생하여도 정상 또는 부분적으로 가능을 수행할 수 있는 System을 의미

> * SPOF(Single Point Of Failure)
> 단일 장애점. 동작하지 않으면 전체 시스템이 작동 X

> * Fail-Over
> 장애극복기능.
> 서버, 컴퓨터, 시스템 네트워크에서 이상이 생겼을 때 극복하는 기능

> * DB Partitioning
> DBMS에서도 데이터의 용량이 커짐에따라 너무 큰 Table이 들어가면서 용량과 성능 측면에서 저하가 발생
> 따라서 Table을 분리하게 되었는데 이를 <b> Data Partitioning</b> 으로 지칭

> * RPC(Remote Procedure Call)
> 한 프로그램이 다른 컴퓨터에 위치하고 있는 프로그램에 서비스를 요청하는 프로토콜

> * 스트림(Stream)
> 일반적으로 데이터,패킷,비트 등의 일련의 연속성을 갖는 흐름

> * 스트라이핑(Striping)
> 스트라이핑은 성능 향상을 위해 단일 파일과 같은 논리적으로 연속된 데이터 세그먼트들이, 물리적으로 여러 개의 장치, 즉 디스크 드라이브 등에 라운드로빈 방식으로 나뉘어 기록될 수 있는 것

> * Write Back 정책
> 이 방식은 CPU에서 메모리에 대한 쓰기 작업 요청 시 캐시에서만 쓰기 작업과 그 변경 사실을 확인할 수 있는 표시를 하여 놓은 후 캐시로부터 해당 블록의 내용이 제거될 때 그 블록을 메인 메모리에 복사함으로써 메인 메모리와 캐시의 내용을 동일하게 유지하는 방식  
